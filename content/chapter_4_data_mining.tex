\chapter{Data Mining}
\label{cha:data_mining}
\begin{itemize}
	\item Input: Preprocessed Data
	\item Output: Model / Patterns
\end{itemize}

\begin{enumerate}
	\item Apply data mining method
	\item Evaluate resulting model / patterns (using P, R, F1, not accuracy)
	\item Iterate:
	\begin{itemize}
		\item Experiment with different parameter settings
		\item Experiment with different alternative methods – Improve preprocessing and feature generation – Combine different methods
	\end{itemize}
\end{enumerate}

\section{Algorithms}
To predict the success of a movie different Algorithms were used:
\begin{itemize}
	\item K-Nearest Neighbor
	\item Naive Bayes
	\item Support Vector Classifier
	\item Neural Net
	\item \textbf{Decision Tree}
	\item \textbf{Random Forest}
\end{itemize}

The following analysis concentrates on the three algorithms with the best results: Random Forest, Decision Tree and Support Vector Classifier.
The first goal of the analysis was to predict the success in five different classes. Since an analysis in that detail with the given data set is very unprecise as seen in table \ref{tab:multi_classifier}, an binary classifier was created. To find the best parameter setting , a gridsearch in combination with a ten-fold cross-validation, scoring the highest F1-score, was applied for each of the classifier.
The achieved F1-scores of the three binary classifier are listed in table \ref{tab:binary_classifier}. At first the classifier were scoring the micro F1-score. Even though the results look promising at the beginning, the classifier were mostly guessing the majority class in this setting. For that reason every classifier was also scored on the macro F1-score, which led to a worse score.

\begin{center}
\begin{table}
	\begin{tabular}{ | p{3.5cm} | p{1.5cm} | p{1.5cm} | p{2cm} | p{2cm} |}
    \hline
    Algorithm & F1 Macro & F1 Micro \\ \hline
    Decision Tree & 36.2\% & 38.6\%  \\ \hline
    Random Forest & 40.4\% & 43.4\%  \\ \hline
    Support Vector Classifier & 37.5\% & 39.1\% \\
    \hline
    \end{tabular}
    \caption{Multi Label Classifier Results} 
    \label{tab:multi_classifier}
\end{table}
\end{center}

\begin{center}
\begin{table}
	\begin{tabular}{ | p{3.5cm} | p{1.5cm} | p{1.5cm} | p{2cm} | p{2cm} |}
    \hline
    Algorithm & F1 Macro & F1 Micro & Downsampled Macro  & Downsampled Micro\\ \hline
    Decision Tree & 56.5\% & 75.5\% & 61.9\% & 63.0\% \\ \hline
    Random Forest & 58.7\% & 76.0\% & 80.6\% & 81.5\% \\ \hline
    Support Vector Classifier & 56.5\% & 75.0\% & 60.7\% & 60.8\% \\
    \hline
    \end{tabular}
    \caption{Binary Classifier Results} 
    \label{tab:binary_classifier}
\end{table}
\end{center}

\subsection{Decision Tree}
Regarding the binary classifier, predicting if a movie is going to be a success, the data set is unbalanced with a proximate ratio of 75\% "successful" and 25\% "unsuccessful". Most of the classifier give bad results with unbalanced data sets. But especially tree-structures can handle unbalanced data sets well, since the hierarchical structure allows them to learn signals from both classes.
But even with a hierarchical structure the algorithm was still just predicting the majority class, while scoring an micro F1 score, as seen in table \ref{tab:binary_classifier}. Since the macro F1 score does not that promising, downsampling was used in the next step to improve the prediction.
Tuples of the majority class were removed to have a balance of 50/50.
Since the downsampled data set is probably too small to learn a good classifier, the next step to improve the algorithm is to upsample the data set. WRITE REST FOR UPSAMPLING HERE

\subsection{Random Forest}
Random Forest builds like the decision tree a hierarchical structure, which can handle unbalanced data sets better than other algorithms. In addition it corrects the overfitting habit of a decision tree. In the gridsearch hyperparameters like the split-criterion, number of features, the minimum sample to split and whether bootstrap samples are used or not are evaluated.
With an downsampled data set the algorithm scored an F1 score micro of 80.6\%, which is an improvement of more than 18\% compared to the decision tree.
SOMETHING ABOUT UPSAMPLING HERE:

\subsection{Support Vector Classifier}

\section{Three best performing algorithms}
\begin{itemize}
	\item Pick best three algos
	\item GridSearch
	\item Why does each classifier perform how it performs (unausgeglichene Klassen, ...)?
\end{itemize}
